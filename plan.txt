# Testing Plan: LLM Components

This document outlines the unit and integration testing strategy for the LLM components located in `src/solace_ai_connector/components/general/llm/litellm/`.

## 1. Overview

The LLM components leverage the LiteLLM library to interact with various Large Language Models. The core components include:
- `LiteLLMBase`: Base class for all LiteLLM interactions, handling configuration, load balancing, and metrics.
- `LiteLLMChatModelBase`: Extends `LiteLLMBase` for chat-specific functionalities, including streaming.
- `LiteLLMChatModel`: A concrete implementation for chat models.
- `LiteLLMChatModelWithHistory`: Extends `LiteLLMChatModelBase` to include conversation history management.
- `LiteLLMEmbeddings`: Component for generating text embeddings.

The testing strategy will involve both unit tests (isolating each class) and integration tests (verifying interactions between components and with a minimal flow context).

## 2. Test Setup and Mocking Strategy

### General
- **Framework:** `pytest` will be used as the testing framework.
- **Fixtures:** `pytest` fixtures will be utilized extensively to provide common test setups, such as:
    - Mock `Message` objects.
    - Default configurations for components.
    - Mocked `ComponentBase` dependencies (e.g., `flow_kv_store`, `flow_lock_manager`).
- **Test Data:** Clear and concise test data for inputs (messages, configurations) and expected outputs.

### Mocking
- **`litellm` Library:** All direct calls to the `litellm` library (e.g., `litellm.completion`, `litellm.embedding`, `litellm.Router`, `litellm.cost_per_token`) will be mocked using `unittest.mock.patch` or `pytest-mock`. This allows testing component logic without actual LLM API calls.
- **`ComponentBase` Dependencies:**
    - `get_config()`: Can be tested by providing a `config` dictionary during component instantiation.
    - `kv_store_get()`, `kv_store_set()`: Mock the `flow_kv_store` object.
    - `send_to_flow()`, `process_post_invoke()`: Mock these methods to assert they are called with correct arguments.
    - `add_timer()`: Mock to verify timer setup.
- **`ChatHistoryHandler`:** For `LiteLLMChatModelWithHistory`, methods inherited or composed from `ChatHistoryHandler` (like `prune_history`, `clear_history_but_keep_depth`, `history_cleanup_timer_callback`) will be tested for their interaction with the KV store and internal logic. Direct calls to these can be made, or their effects can be asserted.
- **Logging and Metrics:** `log` calls can be asserted using `caplog` fixture. `Metrics` interactions will be mocked to verify calls.

## 3. Unit Tests

Unit tests will focus on testing the logic within each class in isolation.

### 3.1. `LiteLLMBase` (`test_litellm_base.py`)
- **Initialization (`__init__`):**
    - Verify correct parsing and storage of configurations: `timeout`, `retry_policy`, `allowed_fails_policy`, `load_balancer_config`, `set_response_uuid_in_user_properties`.
    - Test initialization of `stats` and `_lock_stats`.
- **`init_load_balancer`:**
    - Test successful `litellm.Router` instantiation with default and custom `RetryPolicy` and `AllowedFailsPolicy`.
    - Test handling of invalid `load_balancer_config` (e.g., missing API key/model) raising `ValueError`.
- **`validate_model_config`:**
    - Test with valid configurations.
    - Test with invalid configurations (missing `model`, missing `api_key`) ensuring `ValueError` is raised.
- **`load_balance`:**
    - Mock `self.router.completion`.
    - Test successful response handling.
    - Test handling of `litellm.BadRequestError` (including `ContextWindowExceededError` specific logic).
    - Test handling of other `litellm` exceptions, ensuring they are caught and re-raised as `ValueError`.
- **`context_exceeded_response`:**
    - Verify the structure and content of the `ModelResponse` object returned.
- **Metrics (`send_metrics`, `flush_metrics`, `get_metrics`):**
    - `send_metrics`:
        - Mock `litellm.cost_per_token`.
        - Verify correct calculation of `cost`.
        - Verify all metric types (`PROMPT_TOKENS`, `RESPONSE_TOKENS`, `TOTAL_TOKENS`, `RESPONSE_TIME`, `COST`) are updated in `self.stats` with correct values and timestamps.
        - Test thread safety with `_lock_stats` (if feasible in unit test, otherwise conceptually).
    - `flush_metrics`: Verify `self.stats` is cleared.
    - `get_metrics`: Verify it returns the current `self.stats`.
- **`nack_reaction_to_exception`:**
    - Test with `APIConnectionError` returning `Message_NACK_Outcome.FAILED`.
    - Test with other exception types returning `Message_NACK_Outcome.REJECTED`.

### 3.2. `LiteLLMChatModelBase` (`test_litellm_chat_model_base.py`)
- **Initialization (`__init__`):**
    - Verify correct parsing of `stream_to_flow`, `stream_to_next_component`, `llm_mode`, `stream_batch_size`.
    - Test validation that `stream_to_flow` and `stream_to_next_component` are mutually exclusive (raises `ValueError`).
- **`invoke`:**
    - Test routing to `self.invoke_stream` if `data.get("stream")` is true.
    - Test routing to `self.invoke_stream` if `self.llm_mode == "stream"` and `data.get("stream")` is not present.
    - Test routing to `self.invoke_non_stream` otherwise.
- **`invoke_non_stream`:**
    - Mock `self.load_balance`.
    - Verify correct extraction of `response.choices[0].message.content`.
    - Mock and verify `self.send_metrics` is called with correct token counts and processing time.
    - Test error handling:
        - `APIConnectionError`: Returns `{"content": error_str, "handle_error": True}`.
        - Other `Exception`: Re-raises `ValueError`.
- **`invoke_stream`:**
    - Mock `self.load_balance` to return an iterable of mock stream chunks.
    - Test `set_response_uuid_in_user_properties` correctly sets UUID on the input `message` user properties.
    - Test aggregation of `content` from chunks.
    - Test batching logic with `current_batch` and `self.stream_batch_size`.
    - **Streaming to Flow (`self.stream_to_flow` is set):**
        - Mock `self.send_streaming_message`.
        - Verify it's called for intermediate batches and the final batch with correct `chunk`, `aggregate_result`, `response_uuid`, `first_chunk`, `last_chunk` flags.
        - Verify final result is `{"content": aggregate_result, "response_uuid": response_uuid}`.
    - **Streaming to Next Component (`self.stream_to_next_component` is set):**
        - Mock `self.send_to_next_component`.
        - Verify it's called for intermediate batches with correct flags.
        - Verify final result structure includes `chunk`, `content`, `response_uuid`, `first_chunk=True/False`, `last_chunk=True`, `streaming=True`.
    - Test `send_metrics` call when `chunk.usage` is present.
    - Test error handling:
        - `APIConnectionError`: Returns `{"content": error_str, "response_uuid": ..., "handle_error": True}`.
        - Other `Exception`: Re-raises `ValueError`.
- **`send_streaming_message`:**
    - Mock `self.send_to_flow`.
    - Verify `Message` object is created with correct payload (`chunk`, `content`, `response_uuid`, flags) and user properties.
    - Verify `self.send_to_flow` is called with the target flow name and the new message.
- **`send_to_next_component`:**
    - Mock `self.process_post_invoke`.
    - Verify `Message` object is created with correct payload and user properties.
    - Verify `self.process_post_invoke` is called with the correct result dictionary and the new message.

### 3.3. `LiteLLMChatModel` (`test_litellm_chat_model.py`)
- **Initialization (`__init__`):**
    - Verify `super().__init__` is called with the correct `info` object and other `kwargs`.
    - (Likely minimal tests as most logic is inherited).

### 3.4. `LiteLLMChatModelWithHistory` (`test_litellm_chat_model_with_history.py`)
- **Initialization (`__init__`):**
    - Verify `super().__init__` is called.
    - Verify `history_max_turns`, `history_max_time` are correctly set from config or defaults.
    - Verify `history_key` is generated correctly.
    - Mock `self.add_timer` and verify it's called to set up `history_cleanup`.
- **`invoke`:**
    - Mock `super().invoke` (from `LiteLLMChatModelBase`).
    - Mock `self.kv_store_get`, `self.kv_store_set`, `self.get_lock`.
    - **Session Handling:**
        - Test `ValueError` if `session_id` is not provided.
        - Test creation of new session in history if `session_id` is new.
        - Test retrieval of existing session history.
    - **`clear_history_but_keep_depth`:**
        - Test with `None` (no clearing).
        - Test with `0` (clears all messages for the session).
        - Test with `N > 0` (keeps last N messages).
        - Test invalid values are defaulted to 0.
    - **Message Handling:**
        - Test appending new messages to session history.
        - Test system message replacement logic:
            - If new messages start with a system message and history also starts with one, the history's system message is replaced.
            - Otherwise, new messages (including a new system message if history was empty or didn't start with one) are appended.
    - **History Update:**
        - Verify `history[session_id]["last_accessed"]` is updated.
        - Mock and verify `self.prune_history` (from `ChatHistoryHandler`) is called.
        - Verify assistant's response (from `super().invoke`) is added to `history[session_id]["messages"]`.
        - Verify `self.kv_store_set` is called to save updated history.
    - Ensure `self.get_lock` is used to protect history access.
- **`history_cleanup` (Timer Callback):**
    - This method is typically part of `ChatHistoryHandler`. Test its invocation via the timer if possible, or test the underlying `prune_stale_sessions` logic it calls.
    - Mock `self.kv_store_get`, `self.kv_store_set`.
    - Verify it iterates through sessions and removes stale ones based on `history_max_time`.

### 3.5. `LiteLLMEmbeddings` (`test_litellm_embeddings.py`)
- **Initialization (`__init__`):**
    - Verify `super().__init__` is called with correct `info` and `kwargs`.
- **`invoke`:**
    - Mock `self.router.embedding`.
    - Test with single item in `data["items"]`.
    - Test with multiple items in `data["items"]`.
    - Verify correct extraction of embeddings from the mocked `response.get("data", [])`.
    - Verify the output format `{"embeddings": [...]}`.
    - Test error handling if `self.router.embedding` raises exceptions.

## 4. Integration Tests (`test_litellm_integration.py`)

Integration tests will verify the interaction of LLM components, potentially within a minimal flow setup provided by `solace_ai_connector.test_utils.utils_for_test_files`.

- **Configuration Propagation:**
    - Test that configurations in `LiteLLMBase` (e.g., `timeout`, model params from `load_balancer_config`) are correctly used by `LiteLLMChatModelBase` and `LiteLLMEmbeddings` when making (mocked) LiteLLM calls.
- **Chat Model (Non-Streaming):**
    - Setup `LiteLLMChatModel` within a test flow.
    - Send a message, mock `litellm.completion` at the `litellm` module level.
    - Verify the output message from the flow contains the expected content.
    - Verify metrics are collected.
- **Chat Model (Streaming):**
    - **Stream to Next Component:**
        - Setup a flow: `Input -> LiteLLMChatModel (stream_to_next_component=True) -> MockOutputComponent`.
        - Send a message. Mock `litellm.completion` to return a stream.
        - Verify `MockOutputComponent` receives messages with `chunk`, `content`, `response_uuid`, and correct `first_chunk`/`last_chunk` flags.
    - **Stream to Flow:**
        - Setup `LiteLLMChatModel` with `stream_to_flow="target_flow_name"`.
        - Mock `litellm.completion` to return a stream.
        - Mock the `connector.send_message_to_flow` (or equivalent mechanism for capturing messages sent to other flows).
        - Verify messages with correct streaming payload are sent to "target_flow_name".
- **Chat Model with History:**
    - Setup `LiteLLMChatModelWithHistory` with an in-memory KV store.
    - Send a sequence of messages for the same `session_id`.
    - Mock `litellm.completion`.
    - Verify that the `messages` argument to the mocked `litellm.completion` includes previous turns from the history.
    - Verify history pruning and clearing logic works as expected over several interactions.
- **Embeddings Component:**
    - Setup `LiteLLMEmbeddings` in a test flow.
    - Send a message with text items. Mock `litellm.embedding`.
    - Verify the output message contains the expected embeddings.
- **Error Handling Across Components:**
    - Test how errors from mocked LiteLLM calls (e.g., `APIConnectionError`) are handled by the component and propagated (e.g., NACK outcome, error content in response).
- **Load Balancer Interaction (Conceptual):**
    - If `litellm.Router` allows introspection or if mock models can be configured to simulate failures, test that the router attempts retries or switches models as per its configuration. This might be complex to achieve without a real LiteLLM proxy. Focus on ensuring the router is configured correctly in unit tests.

## 5. Proposed Test File Structure

